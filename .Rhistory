library(UsingR)
data("galton")
library(reshape)
library(reshape2)
long <- melt(galton)
long
g <- ggplot(long,aes(x = value, fill = variable))
g <- g + geom_histogram(colour = "black", binwidth = 1)
g <- g + facet_grid(.~variable)
g
library(UsingR)
data("galton")
plot(galton$parent,galton$child,pch=19,col="blue")
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
plot(as.numeric(as.vector(freqData$parent)),
as.numeric(as.vector(freqData$child)),
pch = 21, col = "black", bg = "lightblue",
cex = .15 * freqData$freq,
xlab = "parent", ylab = "child")
myPlot <- function(beta){
y <- galton$child - mean(galton$child)
x <- galton$parent - mean(galton$parent)
freqData <- as.data.frame(table(x, y))
names(freqData) <- c("child", "parent", "freq")
plot(
as.numeric(as.vector(freqData$parent)),
as.numeric(as.vector(freqData$child)),
pch = 21, col = "black", bg = "lightblue",
cex = .15 * freqData$freq,
xlab = "parent",
ylab = "child"
)
abline(0, beta, lwd = 3)
points(0, 0, cex = 2, pch = 19)
mse <- mean( (y - beta * x)^2 )
title(paste("beta = ", beta, "mse = ", round(mse, 3)))
}
manipulate(myPlot(beta), beta = slider(0.6, 1.2, step = 0.02))
library(manipulate)
install.packages("manipualte")
library(manipulate)
install.packages("manipulate")
library(manipulate)
manipulate(myPlot(beta), beta = slider(0.6, 1.2, step = 0.02))
manipulate(myPlot(beta), beta = slider(0.6, 1.2, step = 0.02))
lm(I(child - mean(child))~ I(parent - mean(parent)) - 1, data = galton)
lm?
?
1
?lm
lm(y(child - mean(child))~ y(parent - mean(parent)) - 1, data = galton)
lm(I(child - mean(child))~ I(parent - mean(parent)) - 1, data = galton)
`
1
2
5l
/]
?
1
q
library(manipulate)
library(UsingR)
data("galton")
y <- galton$child
x <- galton$parent
beta1 <- cor(y, x) *  sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
c(beta0, beta1)
coef(lm(y ~ x))
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
weighted.mean(x,w)
sum(x*w)
sum(x*w)/7
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm
lm(y~x)
lm(y~x -1)
data(mtcars)
lm(mpg~weight,data = mtcars)
lm(mtcars$mpg~mtcars$wt)
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
x -mean
(x -mean(x))/sd(x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y~x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mean(x)
library(UsingR); data(diamond)
data("diamond")
g <- ggplot(diamond,aes(x = carat, y = price),)
g
g <- g + geom_point(size = 6, colour = "black", alpha = 0.2)
g
g <- g + geom_point(size = 5, colour = "blue", alpha = 0.2)
g
g <- geom_smooth(method = "lm", colour = "black")
g
g
g <- geom_smooth(method = "lm", colour = "black", width = x , height = y)
g
x <- runif(100, -3, 3); y <- x + sin(x) + rnorm(100, sd = .2);
plot(x, y); abline(lm(y ~ x))
x <- runif(100, -3, 3); y <- x + sin(x) + rnorm(100, sd = .2);
g <- ggplot(data.frame(x = x, y = y), aes(x=x,y=y))
g <- g + geom_smooth(method = "lm", colour= "black")
g <- g + geom_point(size = 7 , colour = "black", alpha = 0.7)
g <- g + geom_point(size = 5 , colour = "red", alpha = 0.7)
g
require(datasets)
data("swiss")
library(ggplot2)
require(GGally)
install.packages("GGally")
require(GGally)
g <- ggpairs(swiss,lower = list(continuous = "smooth"),params = c(method = "loess"))
g
summary(lm(Fertility ~ .,data ~ swiss))
summary(lm(Fertility ~ . , data = swiss))$coefficients
require(datasets);data(InsectSprays)
require(stats); require(graphics)
boxplot(count ~ spray, data = InsectSprays,
xlab = "Type of spray", ylab = "Insect count",
main = "InsectSprays data", varwidth = TRUE, col = "lightgray")
require(datasets);data(InsectSprays)
require(stats); require(ggplot2)
g = g + ggplot(data = InsectSprays, aes(y = count,x = spray, fill = spray))
g = g + geom_violin(colour = black, size = 2)
g
require(datasets);data(InsectSprays)
require(stats); require(ggplot2)
g =  ggplot(data = InsectSprays, aes(y = count,x = spray, fill = spray))
g = g + geom_violin(colour = black, size = 2)
g
require(datasets);data(InsectSprays)
require(stats); require(ggplot2)
g =  ggplot(data = InsectSprays, aes(y = count,x = spray, fill = spray))
g = g + geom_violin(colour = "black", size = 2)
g
data("mtcars")
fit <- lm(mtcars$mpg ~ factor(mtcars$cyl) + mtcars$wt)
fit
fit2 <- lm(mtcars$mpg ~ factor(mtcars$cyl))
fit2
fit3 <- lm(mtcars$mpg ~ factor(mtcars$cyl)*mtcars$wt)
fit3
result <- anova(fit, fit3, test="Chi")
result$`Pr(>Chi)`
fit4 <-  lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
fit4
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit5 <- lm(y~x)
lm.influence(fit5)$hat[5]
hatvalues(fit5)
dfbetas(fit5)
library(datasets)
library(ggplot2)
data("mtcars")
mtcars$cyl <- as.factor(mtcars$cyl)
mtcars$am <- as.factor(mtcars$am)
mtcars$gear <- as.factor(mtcars$gear)
mtcars$carb <- as.factor(mtcars$carb)
mtcars$vs <- as.factor(mtcars$vs)
head(mtcars)
require(GGally)
g <- ggpairs(mtcars[,c(1,6,7,9)],lower = list(continuous = "smooth"),params = c(method = "loess"))
g
require(GGally)
g <- ggpairs(mtcars[,c(1,6,7,9)],lower = list(continuous = "smooth"),params = c(method = "loess"), binwidth = x)
g
pairs(mtcars[,c(1,6,7,9)], panel=panel.smooth, main="Pair Graph of Motor Trend Car Road Tests")
require(GGally)
g <- ggpairs(mtcars[,c(1,6,7,9)],lower = list(continuous = "smooth"),params = c(method = "loess"), binwidth = x)
g
g  <- ggplot(mtcars, aes(am, mpg))
g<- g + geom_boxplot(aes(fill = am)) + xlab("Transmission Type (0 = Automatic and 1 = Manual)") + ylab("MPG") + ggtitle("Box Plot of MPG Vs Transmission")
g
data("mtcars")
lm(mpg ~ cyl + disp + wt + drat + am, data = mtcars)
f <- lm(mpg ~ cyl + disp + wt + drat + am, data = mtcars)
summary(f)
full_modelfit <- aov(mpg ~ ., data = mtcars)
summary(full_modelfit)
first_modelfit <- aov(mpg ~ cyl + disp + wt + drat + am, data = mtcars)
summary(first_modelfit)
first_modelfit <- aov(mpg ~ cyl + disp + wt + drat + am, data = mtcars, type = 3)
summary(first_modelfit)
summary(mtcars)
View(mtcars)
require(ggplot2)
diagPlot<-function(model){
p1<-ggplot(model, aes(.fitted, .resid))+geom_point()
p1<-p1+stat_smooth(method="loess")+geom_hline(yintercept=0, col="red", linetype="dashed")
p1<-p1+xlab("Fitted values")+ylab("Residuals")
p1<-p1+ggtitle("Residual vs Fitted Plot")+theme_bw()
p2<-ggplot(model, aes(qqnorm(.stdresid)[[1]], .stdresid))+geom_point(na.rm = TRUE)
p2<-p2+geom_abline(aes(qqline(.stdresid)))+xlab("Theoretical Quantiles")+ylab("Standardized Residuals")
p2<-p2+ggtitle("Normal Q-Q")+theme_bw()
p3<-ggplot(model, aes(.fitted, sqrt(abs(.stdresid))))+geom_point(na.rm=TRUE)
p3<-p3+stat_smooth(method="loess", na.rm = TRUE)+xlab("Fitted Value")
p3<-p3+ylab(expression(sqrt("|Standardized residuals|")))
p3<-p3+ggtitle("Scale-Location")+theme_bw()
p4<-ggplot(model, aes(seq_along(.cooksd), .cooksd))+geom_bar(stat="identity", position="identity")
p4<-p4+xlab("Obs. Number")+ylab("Cook's distance")
p4<-p4+ggtitle("Cook's distance")+theme_bw()
p5<-ggplot(model, aes(.hat, .stdresid))+geom_point(aes(size=.cooksd), na.rm=TRUE)
p5<-p5+stat_smooth(method="loess", na.rm=TRUE)
p5<-p5+xlab("Leverage")+ylab("Standardized Residuals")
p5<-p5+ggtitle("Residual vs Leverage Plot")
p5<-p5+scale_size_continuous("Cook's Distance", range=c(1,5))
p5<-p5+theme_bw()+theme(legend.position="bottom")
p6<-ggplot(model, aes(.hat, .cooksd))+geom_point(na.rm=TRUE)+stat_smooth(method="loess", na.rm=TRUE)
p6<-p6+xlab("Leverage hii")+ylab("Cook's Distance")
p6<-p6+ggtitle("Cook's dist vs Leverage hii/(1-hii)")
p6<-p6+geom_abline(slope=seq(0,3,0.5), color="gray", linetype="dashed")
p6<-p6+theme_bw()
#cdPlot=p4,
#cvlPlot=p6
return(list(rvfPlot=p1, qqPlot=p2, sclLocPlot=p3,  rvlevPlot=p5 ))
# return(list(rvfPlot=p1, qqPlot=p2, sclLocPlot=p3, cdPlot=p4, rvlevPlot=p5, cvlPlot=p6))
}
diagnosticPlots <- diagPlot(second_modelfit)
second_modelfit <- lm(mpg ~ cyl  + wt + am, data = mtcars)
summary(second_modelfit)
diagnosticPlots <- diagPlot(second_modelfit)
require(gridExtra)
grid.arrange(diagnosticPlots,ncol=2)
require(ggplot2)
diagPlot<-function(model){
p1<-ggplot(model, aes(.fitted, .resid))+geom_point()
p1<-p1+stat_smooth(method="loess")+geom_hline(yintercept=0, col="red", linetype="dashed")
p1<-p1+xlab("Fitted values")+ylab("Residuals")
p1<-p1+ggtitle("Residual vs Fitted Plot")+theme_bw()
p2<-ggplot(model, aes(qqnorm(.stdresid)[[1]], .stdresid))+geom_point(na.rm = TRUE)
p2<-p2+geom_abline(aes(qqline(.stdresid)))+xlab("Theoretical Quantiles")+ylab("Standardized Residuals")
p2<-p2+ggtitle("Normal Q-Q")+theme_bw()
p3<-ggplot(model, aes(.fitted, sqrt(abs(.stdresid))))+geom_point(na.rm=TRUE)
p3<-p3+stat_smooth(method="loess", na.rm = TRUE)+xlab("Fitted Value")
p3<-p3+ylab(expression(sqrt("|Standardized residuals|")))
p3<-p3+ggtitle("Scale-Location")+theme_bw()
p4<-ggplot(model, aes(seq_along(.cooksd), .cooksd))+geom_bar(stat="identity", position="identity")
p4<-p4+xlab("Obs. Number")+ylab("Cook's distance")
p4<-p4+ggtitle("Cook's distance")+theme_bw()
p5<-ggplot(model, aes(.hat, .stdresid))+geom_point(aes(size=.cooksd), na.rm=TRUE)
p5<-p5+stat_smooth(method="loess", na.rm=TRUE)
p5<-p5+xlab("Leverage")+ylab("Standardized Residuals")
p5<-p5+ggtitle("Residual vs Leverage Plot")
p5<-p5+scale_size_continuous("Cook's Distance", range=c(1,5))
p5<-p5+theme_bw()+theme(legend.position="bottom")
p6<-ggplot(model, aes(.hat, .cooksd))+geom_point(na.rm=TRUE)+stat_smooth(method="loess", na.rm=TRUE)
p6<-p6+xlab("Leverage hii")+ylab("Cook's Distance")
p6<-p6+ggtitle("Cook's dist vs Leverage hii/(1-hii)")
p6<-p6+geom_abline(slope=seq(0,3,0.5), color="gray", linetype="dashed")
p6<-p6+theme_bw()
#cdPlot=p4,
#cvlPlot=p6
return(p1,p2,p3,p5)
#return(list(rvfPlot=p1, qqPlot=p2, sclLocPlot=p3,  rvlevPlot=p5 ))
# return(list(rvfPlot=p1, qqPlot=p2, sclLocPlot=p3, cdPlot=p4, rvlevPlot=p5, cvlPlot=p6))
}
diagnosticPlots <- diagPlot(second_modelfit)
require(ggplot2)
diagPlot<-function(model){
p1<-ggplot(model, aes(.fitted, .resid))+geom_point()
p1<-p1+stat_smooth(method="loess")+geom_hline(yintercept=0, col="red", linetype="dashed")
p1<-p1+xlab("Fitted values")+ylab("Residuals")
p1<-p1+ggtitle("Residual vs Fitted Plot")+theme_bw()
p2<-ggplot(model, aes(qqnorm(.stdresid)[[1]], .stdresid))+geom_point(na.rm = TRUE)
p2<-p2+geom_abline(aes(qqline(.stdresid)))+xlab("Theoretical Quantiles")+ylab("Standardized Residuals")
p2<-p2+ggtitle("Normal Q-Q")+theme_bw()
p3<-ggplot(model, aes(.fitted, sqrt(abs(.stdresid))))+geom_point(na.rm=TRUE)
p3<-p3+stat_smooth(method="loess", na.rm = TRUE)+xlab("Fitted Value")
p3<-p3+ylab(expression(sqrt("|Standardized residuals|")))
p3<-p3+ggtitle("Scale-Location")+theme_bw()
p4<-ggplot(model, aes(seq_along(.cooksd), .cooksd))+geom_bar(stat="identity", position="identity")
p4<-p4+xlab("Obs. Number")+ylab("Cook's distance")
p4<-p4+ggtitle("Cook's distance")+theme_bw()
p5<-ggplot(model, aes(.hat, .stdresid))+geom_point(aes(size=.cooksd), na.rm=TRUE)
p5<-p5+stat_smooth(method="loess", na.rm=TRUE)
p5<-p5+xlab("Leverage")+ylab("Standardized Residuals")
p5<-p5+ggtitle("Residual vs Leverage Plot")
p5<-p5+scale_size_continuous("Cook's Distance", range=c(1,5))
p5<-p5+theme_bw()+theme(legend.position="bottom")
p6<-ggplot(model, aes(.hat, .cooksd))+geom_point(na.rm=TRUE)+stat_smooth(method="loess", na.rm=TRUE)
p6<-p6+xlab("Leverage hii")+ylab("Cook's Distance")
p6<-p6+ggtitle("Cook's dist vs Leverage hii/(1-hii)")
p6<-p6+geom_abline(slope=seq(0,3,0.5), color="gray", linetype="dashed")
p6<-p6+theme_bw()
#cdPlot=p4,
#cvlPlot=p6
return(list(p1,p2,p3,p5))
#return(list(rvfPlot=p1, qqPlot=p2, sclLocPlot=p3,  rvlevPlot=p5 ))
# return(list(rvfPlot=p1, qqPlot=p2, sclLocPlot=p3, cdPlot=p4, rvlevPlot=p5, cvlPlot=p6))
}
diagnosticPlots <- diagPlot(second_modelfit)
grid.arrange(diagnosticPlots,ncol=2)
do.call("grid.arrange", c(diagnosticPlots, ncol=2))
sum(abs(second_modelfit)
sum((abs(dfbetas(amIntWtModel)))>1)
sum((abs(dfbetas(second_modelfit)))>1)
hyp <- t.test(mpg ~ am, data = mtcars)
summary(hyp)
hyp <- t.test(mpg ~ am)
summary(hyp)
hyp <- t.test(mpg ~ am, data = mtcars)
hyp
hyp$p.value
hyp$estimate
24.39231 - 17.14737
library(kernlab)
install.packages(kernlab)
install.packages("kernlab")
library(kernlab)
data(spam)
head(spam)
plot(density(spam$your[spam$type=="nonspam"]),
col="blue",main="",xlab="Frequency of 'your'")
lines(density(spam$your[spam$type=="spam"]),col="red")
999 + 98901
library(caret)
install.packages("caret")
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y = spam$type, p =0.75, list = FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
set.seed(32343)
modelFit <- train(type ~.,data=training, method="glm")
install.packages('e1071', dependencies=TRUE)
install.packages('caret', dependencies = TRUE)
install.packages("caret", dependencies = TRUE)
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y = spam$type, p =0.75, list = FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
set.seed(32343)
modelFit <- train(type ~.,data=training, method="glm")
modelFit
modelFit$finalModel
predictions <- predict(modelFit,newdata=testing)
predictions
confusionMatrix(predictions,testing$type)
set.seed(32323)
folds <- createFolds(y=spam$type,k=10,
list=TRUE,returnTrain=TRUE)
sapply(folds,length)
folds[[1]][1:10]
args(train.default)
modelFit
install.packages("ISLR")
library(ISLR); library(ggplot2); library(caret);
data(Wage)
summary(Wage)
inTrain <- createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training); dim(testing)
featurePlot(x=training[,c("age","education","jobclass")],
y = training$wage,
plot="pairs")
qplot(age,wage,data=training)
qplot(age,wage,colour=jobclass,data=training)
qq <- qplot(age,wage,colour=education,data=training)
qq +  geom_smooth(method='lm',formula=y~x)
cutWage <- cut2(training$wage,g=3)
install.packages("Hmisc ")
library(Hmisc)
cutWage <- cut2(training$wage,g=3)
table(cutWage)
install.packages("splines")
install.packages("splines")
install.packages("splines")
install.packages("splines")
library(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
data(concrete)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
histogram(training)
xnames <- colnames(concrete)[1:8]
featurePlot(x=training[, xnames], y=training$CompressiveStrength, plot="pairs")
index <- seq_along(1:nrow(training))
ggplot(data=training, aes(x=index, y=CompressiveStrength)) + geom_point() +
theme_bw()
ggplot(data=training, aes(x=Superplasticizer)) + geom_histogram() + theme_bw()
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(ss, method='pca', thresh = 0.8, outcome = training$diagnosis)
preProc
model1 <- train(ss, testing$diagnosis, method='glm')
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
C1 <- confusionMatrix(predictions, testing$diagnosis)
c1
C1
A1 <- C1$overall[1]
modelFit2 <- train(training$diagnosis ~ ., method = "glm", data = training,
preProcess = "pca",
Control = trainControl(preProcOptions = list(thresh = 0.8)))
set.seed(3433)
IL <- grep("^IL", colnames(training), value=TRUE)
ILpredictors <- predictors[, IL]
df <- data.frame(diagnosis, ILpredictors)
inTrain <- createDataPartition(df$diagnosis, p=3/4)[[1]]
training <- df[inTrain, ]
testing <- df[-inTrain, ]
modelFit <- train(diagnosis ~ ., method="glm", data=training)
predictions <- predict(modelFit, newdata=testing)
C1 <- confusionMatrix(predictions, testing$diagnosis)
C1
acc1 <- C1$overall[1]
modelFit1 <- train(training$diagnosis ~ .,
method="glm",
preProcess="pca",
data=training,
trControl=trainControl(preProcOptions=list(thresh=0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
C2
acc2 <- C2$overall[1]
acc2
install.packages("AppliedPredictiveModeling")
install.packages("ElemStatLearn")
install.packages("pgmm")
install.packages("rpart")
library(caret,  quietly=TRUE)
set.seed(987)
setwd("~/Coursera/practical machine learning/course proj 1")
training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
library(caret,  quietly=TRUE)
library(randomForest,  quietly=TRUE)
library(rpart,  quietly=TRUE)
library(rpart.plot,  quietly=TRUE)
set.seed(987)
setwd("~/Coursera/practical machine learning/course proj 1")
training <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
testing <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
install.packages("rpart.plot")
library(caret,  quietly=TRUE)
library(randomForest,  quietly=TRUE)
library(rpart,  quietly=TRUE)
library(rpart.plot,  quietly=TRUE)
set.seed(987)
setwd("~/Coursera/practical machine learning/course proj 1")
training <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
testing <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
#deleting columns with NA
training<-training[,colSums(is.na(training)) == 0]
testing <-testing[,colSums(is.na(testing)) == 0]
## removing irrelevant coloumns like stamp
training   <-training[,-c(1:7)]
testing <-testing[,-c(1:7)]
View(testing)
View(testing)
library(caret,  quietly=TRUE)
library(randomForest,  quietly=TRUE)
library(rpart,  quietly=TRUE)
library(rpart.plot,  quietly=TRUE)
set.seed(987)
setwd("~/Coursera/practical machine learning/course proj 1")
training <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
testing <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
#deleting columns with NA
training<-training[,colSums(is.na(training)) == 0]
testing <-testing[,colSums(is.na(testing)) == 0]
## removing irrelevant coloumns like stamp
training   <-training[,-c(1:7)]
testing <-testing[,-c(1:7)]
dim(training)
dim(testing)
head(training, n=5)
head(testing, n =5)
library(rpart.plot)
iinstall.packages("rpart.plot")
install.packages("rpart.plot")
library(rpart.plot)
model1 <- rpart(ddata = training_part, classe ~ ., method = "class")
model1 <- rpart(data = training_part, classe ~ ., method = "class")
inTrain <- createDataPartition(y=training$classe, p=0.75, list=FALSE)
training_part <- training[inTrain, ]
testing_part <- training[-inTrain, ]
dim(training_part); dim(testing_part)
model1 <- rpart(data = training_part, classe ~ ., method = "class")
model1
model2 <- randomForest(classe ~. , data=training_part, method="class")
model2 <- randomForest(classe ~. , data=training_part, method="class")
prediction2 <- predict(model2, testing_part, type = "class")
confusionMatrix(prediction2, testing_part$classe)
predictfinal <- predict(model2, testing, type="class")
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(predictfinal)
